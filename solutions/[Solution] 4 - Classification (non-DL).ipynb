{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 4: Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we will exploit quantitative measurements in order to retreive different phenotypic groups present in a large image dataset. The end goal will be to group samples (whether full images or individual objects) into different classes, a process referred to as *classification*. We will here review commonly-used (non-deep-learning) strategies for classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import imageio.v2 as imageio\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.rcParams['figure.dpi'] = 200"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part, we will use pandas to handle numerical data (https://pandas.pydata.org/) and seaborn to generate cute plots (). Feel free to consult the extensive documentation available on their websites if you want to know more about these libraries!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following on what we did in notebook 3 - Quantification, we will again work with feature matrices extracted from the BBBC010 dataset featuring dead and live *C. elegans* worms. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.1** Run the lines below to load and display the feature matrix for the entire BBBC010 dataset. Note that features are here reported *per-image*: they correspond to the average value of any given feature across all instances present in the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bbbc010_img_feats = pd.read_csv('../data/Part_4/BBBC010/bbbc010_image_features.csv')\n",
    "bbbc010_img_feats.set_index('image_id', inplace = True)\n",
    "\n",
    "display(bbbc010_img_feats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.2** As you may have noticed when inspecting the feature matrix above, different features have very different range of values. To make sure features with higher values do not \"overwhelm\" features with lower values, it is useful to first rescale them all into the [0, 1] range. Run the lines below to carry this out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "bbbc010_img_feats[:] = scaler.fit_transform(bbbc010_img_feats[:])\n",
    "\n",
    "display(bbbc010_img_feats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.3** In order to evaluate the quality of our classification attempts, we need a ground truth to compare to. Run the lines below to load and display the ground truth label for each image in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bbbc010_img_gt = pd.read_csv('../data/Part_4/BBBC010/bbbc010_image_ground_truth.csv')\n",
    "bbbc010_img_gt.set_index('image_id', inplace = True)\n",
    "\n",
    "display(bbbc010_img_gt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.4** To simplify processing later on, it is useful to have a \"labelled\" version of the feature matrix. Run the lines below to merge the feature matrix and the ground truth labels into a single data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bbbc010_img = bbbc010_img_feats.join(bbbc010_img_gt)\n",
    "\n",
    "display(bbbc010_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Feature selection and dimensionality reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we have seen in notebook 3- Quantification, it is hard to find a single feature able to discriminate between dead and live worms. The feature matrix we are working with has thus been built by putting together an extensive collection of 33 measurements capturing shape and intensity, in the hope that all of these features considered together can capture the difference between the 'dead' and 'live' phenotypes. \n",
    "\n",
    "While it is clear that more than a single feature is needed, some of the features may however be more informative than others. Among the 33 features considered, some may in fact be entirely uninformative. Revealing which features are relevant and which aren't and making sure that our feature matrix is not too redundant is the job of feature selection and dimensionality reduction methods, as we shall see now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.1** In order to get a first impression of whether each of the feature in our matrix is informative, we can investigate their distributions, for instance by looking at violin plots produced by running the lines below. We know that our dataset is composed of two classes (dead and live), and are therefore mostly interested in features that have a bimodal distribution. Can you spot features that seem to be uninformative?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a violin plot for each feature\n",
    "for f in bbbc010_img_feats.keys():\n",
    "    sb.violinplot(y = bbbc010_img_feats[f])\n",
    "\n",
    "    plt.show()\n",
    "    # The Euler number is constant and therefore useless. A bunch of other features (e.g., intensity_max, \n",
    "    # perimeter) are normally-distributed and therefore of limited use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.2** Variance is a a good proxy for how informative a feature is. Intuitively, any feature that remains constant (or almost constant) throughout the dataset is unlikely to be helpful when it comes to classifying. A simple way to automate the identification of such low-variance features is to set a variance threshold and spot any feature that varies less than that. Run the lines below to load the VarianceThreshold method from scikit-learn and apply it to our feature matrix. Does the result corroboate your observations from 2.1?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import VarianceThreshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define what is considered as too low variance\n",
    "epsilon = 1e-5\n",
    "variance_thresh = VarianceThreshold(threshold = epsilon)\n",
    "\n",
    "# Apply variance threshold\n",
    "variance_thresh.fit(bbbc010_img_feats)\n",
    "\n",
    "# Display low-variance features\n",
    "print(list(bbbc010_img_feats.columns[~variance_thresh.get_support()]))\n",
    "# We do retreive the Euler number, that we identified in 2.1 as being constant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.3** In addition to being informative on their own, we would also like features in our matrix to not be redundant. In other words, we do not want features to correlate too much with each other. This can be investigated by looking at the correlation matrix below. Do you identify groups of features that are heavily correlated? Do you intuitively understand why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the correlation matrix\n",
    "correlation_matrix = bbbc010_img_feats.corr()\n",
    "\n",
    "# Generate a mask for the upper triangle\n",
    "upper_mask = np.triu(correlation_matrix.corr())\n",
    "\n",
    "# Visualize the result as a heatmap\n",
    "palette = sb.diverging_palette(20, 220, n = 256)\n",
    "\n",
    "sb.heatmap(correlation_matrix, xticklabels=correlation_matrix.columns, yticklabels=correlation_matrix.columns, vmin=-1, vmax=1, center=0, cmap=palette,\n",
    "           square=True, mask=upper_mask)\n",
    "\n",
    "plt.show()\n",
    "# Groups of features that capture the same aspect are correlated. Examples include solidity with area_convex, \n",
    "# area_filled with area, extent with area_bbox, feret_diameter_max with axis_minor_length and axis_major_length."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.4** When ground-truth class labels are available, Mutual Information provides a formal way of evaluating whether features are predictive of the class label or not. It ranges between 0 when the feature is independent from the class label, to positive values when the feature is dependent on the class, with higher values indicating a stronger dependency. Mutual Information is based on the notion of Shannon entropy, a core theoretical concept in information theory (https://en.wikipedia.org/wiki/Mutual_information). \n",
    "\n",
    "Run the lines below to compute the Mutual Information of the features in our matrix and visualize the result. Which ones do you identify as being informative? Does the result corroboate your observations from 2.2?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import mutual_info_classif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create matrix of data and labels\n",
    "X = bbbc010_img.loc[:, bbbc010_img.columns != 'label']\n",
    "Y = bbbc010_img.loc[:, 'label']\n",
    "\n",
    "# Compute feature importance\n",
    "importance = mutual_info_classif(X,Y)\n",
    "\n",
    "# Visualize the result\n",
    "feature_importance = pd.Series(importance, bbbc010_img.columns[0: len(bbbc010_img.columns)-1])\n",
    "feature_importance.plot(kind='barh', color='teal')\n",
    "\n",
    "plt.show()\n",
    "# The Euler number, that we already identified as constant, is identified here as well to be uninformative. \n",
    "# Other features such as moments_weighted_hu-2 and moments_weighted_hu-6 are flagged as uninformative. \n",
    "# Interestingly, they appear to be uncorrelated with most other measurements in 2.3, but also seem to have\n",
    "# highly multimodal distributions, as seen in 2.1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.5** Beyond selecting individual features, another way to reduce the dimensionality of our feature matrix is to try and find a few combinations of features that can explain most of the variability present in the matrix. This is the idea behind the famous principal component analysis (PCA, https://en.wikipedia.org/wiki/Principal_component_analysis). \n",
    "\n",
    "Run the lines below to 1) extract the first N principal components of our feature matrix, and 2) plot the  variance that they are able to explain. Based on this, how many principal components do you think is sufficient to analyze this dataset? How does that compare to the number of features we initially had, and how does that relate to your observations in 2.1 and 2.3?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix the number of principal components to be extracted\n",
    "N = 10\n",
    "\n",
    "# Perform PCA\n",
    "pca = PCA(n_components = N)\n",
    "pca.fit(bbbc010_img_feats)\n",
    "\n",
    "# Retreive the % of variance explained by each Principal Component\n",
    "percent_variance_explained = np.round(pca.explained_variance_ratio_* 100, decimals=2)\n",
    "print(\"Percentage of variance explained with \"+str(N)+\" principal components: \"+str(np.sum(percent_variance_explained)))\n",
    "\n",
    "# Visualize the explained variance (scree plot)\n",
    "pc_labels = ['PC'+str(x) for x in range(1, len(percent_variance_explained)+1)]\n",
    "plt.bar(x = range(1,len(percent_variance_explained)+1), height = percent_variance_explained, \n",
    "        tick_label = pc_labels)\n",
    "\n",
    "plt.ylabel('% Explained Variance')\n",
    "plt.xlabel('Principal Component')\n",
    "plt.title('Scree Plot')\n",
    "\n",
    "plt.show()\n",
    "# 99% of the variance is explained with 17 principal components, so we certainly don't need more than 17 features\n",
    "# to fully characterize this dataset. It means that almost half of the features in our matrix (composed of 33 in \n",
    "# total) can be dropped. This is consistent with our observations that several features are normally distributed\n",
    "# (2.1) and that many of them exhibit strong correlations with each other (2.3)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.6** PCA, in 2.5, finds a lower-dimensional subspace that maximizes the variation in the whole data set, without accounting for classes. Here, since we also have ground truth labels, we can alternatively apply Linear Discriminant Analysis (LDA, https://en.wikipedia.org/wiki/Linear_discriminant_analysis). LDA aims at retreiving a feature subspace that maximizes the variance between our two classes (*inter-class* variance) while minimizing the variance within each given class (*intra-class* variance). \n",
    "\n",
    "Run the lines below to 1) perform LDA on the feature matrix, and 2) plot the data according to the linear discriminant value. Below, we also provides code to plot the data according to the value of their first principal component. Which one of LDA or PCA do you think is better in this case? Can you guess why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create matrix of data and labels\n",
    "X = bbbc010_img.loc[:, bbbc010_img.columns != 'label']\n",
    "Y = bbbc010_img.loc[:, 'label']\n",
    "\n",
    "# Perform LDA\n",
    "lda = LinearDiscriminantAnalysis()\n",
    "bbbc010_img_lda = lda.fit_transform(X, Y)\n",
    "\n",
    "# Visualize the result\n",
    "plt.hist(bbbc010_img_lda)\n",
    "\n",
    "plt.xlabel('LD value')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Linear Discriminant Analysis')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform PCA\n",
    "pca = PCA(n_components = 1)\n",
    "pca.fit(bbbc010_img_feats)\n",
    "\n",
    "# Transform the data into their PC\n",
    "bbbc010_img_pca = pca.transform(bbbc010_img_feats) \n",
    "\n",
    "# Visualize the result\n",
    "plt.hist(bbbc010_img_pca)\n",
    "\n",
    "plt.xlabel('1st PC value')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Principal Component Analysis')\n",
    "\n",
    "plt.show()\n",
    "# It looks like LDA is maginally better than PCA, as it allows identifying two well-separated groups."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Unsupervised classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have identified strategies to \"clean\" our feature matrix, we can dig into the actual classification and try to automatically retreive which group each sample belongs to. We will first look at *unsupervised* classification methods that do not require knowledge of the underlying class labels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.1** K-means clustering (https://en.wikipedia.org/wiki/K-means_clustering) is perhaps the most well-know unsupervised classification algorithm. In a nuthsell, K-means works as follows: 1) K data points are randomly selected as class centroids, 2) all other data points are associated to the centroid they are the closest to, 3) the class centroids are recomputed as the average of the points assigned to them. This procedure is iteratively repeated until convergence, revealing the \"true\" classes present in the dataset.\n",
    "\n",
    "Run the lines below to retreive the class label obtained by running 2-means clustering on the LDA-transformed data. Then, adapt the code to run 2-means clustering on the PCA-transformed data, using the 1st principal component only (see 2.6). Do you see a difference?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# There are two classes in this dataset\n",
    "K = 2\n",
    "\n",
    "# Perform K-means on LDA-transformed data\n",
    "kmeans = KMeans(n_clusters=K, random_state=5)\n",
    "kmeans.fit(bbbc010_img_lda)\n",
    "\n",
    "# Retreive the cluster label and display it\n",
    "kmeans_labels = kmeans.predict(bbbc010_img_lda)\n",
    "\n",
    "kmeans_labels_lda = np.empty((len(kmeans_labels)), dtype='object')\n",
    "kmeans_labels_lda[kmeans_labels == 0] = 'dead'\n",
    "kmeans_labels_lda[kmeans_labels == 1] = 'live'\n",
    "\n",
    "print(kmeans_labels_lda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform K-means on PCA-transformed data (1st PC)\n",
    "kmeans = KMeans(n_clusters=K, random_state=5)\n",
    "kmeans.fit(bbbc010_img_pca)\n",
    "\n",
    "# Retreive the cluster label and display it\n",
    "kmeans_labels = kmeans.predict(bbbc010_img_pca)\n",
    "\n",
    "kmeans_labels_pca = np.empty((len(kmeans_labels)), dtype='object')\n",
    "kmeans_labels_pca[kmeans_labels == 0] = 'dead'\n",
    "kmeans_labels_pca[kmeans_labels == 1] = 'live'\n",
    "\n",
    "print(kmeans_labels_pca)\n",
    "# It is heard to see a difference just by looking at the label vector, we need better ways to quantify performance\n",
    "# as we shall see in part 4."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.2** In an unsupervised context we do not have access to class labels. A good way to investigate whether the two clusters we retreived are really two distinct classes or whether they overlap is the Kolmogorov–Smirnov test (KS, https://en.wikipedia.org/wiki/Kolmogorov%E2%80%93Smirnov_test). The KS test checks whether the samples from each clusters are drawn from the same distribution or not, giving us an indication of whether two classes indeed exist. The KS test outputs two readouts, the KS statistic (ranging from 0 to 1) and a p-value. A small KS statistic or a high p-value indicate that we cannot reject the null hypothesis stating that the two distributions are equivalent. \n",
    "\n",
    "Run the lines below to 1) perform the KS test on the clusters retreived in 3.1, and 2) output the KS statistic  and p-value. What is your conclusion? Adapt the code to run the same analysis on the PCA-transformed data, using the 1st principal component only (see 2.6). Do reach the same conclusion?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import ks_2samp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retreive data clusters\n",
    "cluster_1 = np.squeeze(bbbc010_img_lda[kmeans_labels_lda == 'dead'])\n",
    "cluster_2 = np.squeeze(bbbc010_img_lda[kmeans_labels_lda == 'live'] )\n",
    " \n",
    "# Perform KS test on LDA-transformed data\n",
    "stat, pvalue = ks_2samp(cluster_1, cluster_2)\n",
    "\n",
    "print('Kolmogorov–Smirnov statistic (LDA):', stat)\n",
    "print('p-value (LDA):', pvalue)\n",
    "# The two classes indeed form different distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retreive data clusters\n",
    "cluster_1 = np.squeeze(bbbc010_img_pca[kmeans_labels_pca == 'dead'])\n",
    "cluster_2 = np.squeeze(bbbc010_img_pca[kmeans_labels_pca == 'live'] )\n",
    " \n",
    "# Perform KS test on PCA-transformed data (1st PC)\n",
    "stat, pvalue = ks_2samp(cluster_1, cluster_2)\n",
    "\n",
    "print('Kolmogorov–Smirnov statistic (PCA):', stat)\n",
    "print('p-value (PCA):', pvalue)\n",
    "# The two classes indeed form different distributions. We reach the same conclusion as with LDA-transformed data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Evaluating classification performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although we ignored them when carrying out unsupervised classification in part 3, we do in fact have access to class labels. This information can be used to quantitatively evaluate the performance of our classification algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.metrics as skm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4.1** The preferred way of reporting the performance of a classifier is the so-called confusion matrix (https://en.wikipedia.org/wiki/Confusion_matrix). Each entry in the matrix tells us how many data points were predicted to belong to the class given by the row index, while actually being from the class indicated by the column index. The confusion matrix therefore allows us to immediately spot the amount of samples that were classified correctly (in the diagonal) or incorrectly (outside of the diagonal).\n",
    "\n",
    "Run the lines below to compute and display the confusion matrix of the labels predicted by K-means on the LDA-transformed data. Then, adapt the code to obtain the confusion matrix of the labels predicted by K-means on the PCA-transformed data, using the 1st principal component only (see 2.6). Do notice a difference?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retreive predictions (LDA-transformed data) and ground truth \n",
    "predictions = kmeans_labels_lda\n",
    "ground_truth = list(bbbc010_img.loc[:, 'label'])\n",
    "\n",
    "# Compute the confusion matrix\n",
    "confusion_matrix = skm.confusion_matrix(ground_truth, predictions, labels = np.unique(ground_truth))\n",
    "\n",
    "# Visualize the result\n",
    "disp = skm.ConfusionMatrixDisplay(confusion_matrix = confusion_matrix, display_labels = np.unique(ground_truth))\n",
    "disp.plot()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retreive predictions (PCA-transformed data, 1st PC) and ground truth\n",
    "predictions = kmeans_labels_pca\n",
    "ground_truth = list(bbbc010_img.loc[:, 'label'])\n",
    "\n",
    "# Compute the confusion matrix\n",
    "confusion_matrix = skm.confusion_matrix(ground_truth, predictions, labels = np.unique(ground_truth))\n",
    "\n",
    "# Visualize the result\n",
    "disp = skm.ConfusionMatrixDisplay(confusion_matrix = confusion_matrix, display_labels = np.unique(ground_truth))\n",
    "disp.plot()\n",
    "\n",
    "plt.show()\n",
    "# We have one live sample misclassified as dead!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4.2** Summarizing classification performance as a single number instead of a matrix is sometimes desirable, but often difficult as no metric can captures every single aspect of a classification problem one may want to assess. A large variety of classification metrics are available depending on what one wants to focus on (https://en.wikipedia.org/wiki/Evaluation_of_binary_classifiers).\n",
    "\n",
    "Run the lines below to compute and display a handful of classification performance metrics evaluating the quality of the labels predicted by K-means on the LDA-transformed data. Then, adapt the code to evaluate the quality of the labels predicted by K-means on the PCA-transformed data, using the 1st principal component only (see 2.6). Do you notice a difference?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retreive predictions (LDA-transformed data) and ground truth\n",
    "predictions = kmeans_labels_lda\n",
    "ground_truth = list(bbbc010_img.loc[:, 'label'])\n",
    "\n",
    "# Compute a handful of classification metrics and display them\n",
    "accuracy = skm.accuracy_score(ground_truth, predictions)\n",
    "print('Accuracy: '+str(accuracy))\n",
    "\n",
    "precision = skm.precision_score(ground_truth, predictions, pos_label= 'live')\n",
    "print('Precision: '+str(precision))\n",
    "\n",
    "recall = skm.recall_score(ground_truth, predictions, pos_label= 'live')\n",
    "print('Recall: '+str(recall))\n",
    "\n",
    "f1_score = skm.f1_score(ground_truth, predictions, pos_label= 'live')\n",
    "print('F1 score: '+str(f1_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retreive predictions (PCA-transformed data, 1st PC) and ground truth\n",
    "predictions = kmeans_labels_pca\n",
    "ground_truth = list(bbbc010_img.loc[:, 'label'])\n",
    "\n",
    "# Compute a handful of classification metrics and display them\n",
    "accuracy = skm.accuracy_score(ground_truth, predictions)\n",
    "print('Accuracy: '+str(accuracy))\n",
    "\n",
    "precision = skm.precision_score(ground_truth, predictions, pos_label= 'live')\n",
    "print('Precision: '+str(precision))\n",
    "\n",
    "recall = skm.recall_score(ground_truth, predictions, pos_label= 'live')\n",
    "print('Recall: '+str(recall))\n",
    "\n",
    "f1_score = skm.f1_score(ground_truth, predictions, pos_label= 'live')\n",
    "print('F1 score: '+str(f1_score))\n",
    "\n",
    "# As we guessed already in 2.6, LDA is marginally better than the 1st PC when it comes to discriminating \n",
    "# between the two classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Supervised classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having access to class labels, we can also explore *supervised* classification methods. These classical (i.e., not \"deep\") machine learning algorithms use provided labels to learn how features should be combined in order to best discriminate between classes. Once trained, they can be run in *inference mode* on unlabeled data and provide class predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5.1** A trained classifier is supposed to be able to accurately classify unseen data points, but how to assess whether it does so well enough? The trick is to split our labelled data, prior to training, into a so-called *training* and *test* sets. The data and labels of the training set are visible to the classified and used for the actual training, while the test set remains hidden during that time. Once the classifier is trained, it will have to predict labels for the test set data. These predicted labels will be compared to the known ground-truth ones, thus providing an estimation of the predictive power of the trained algorithm. \n",
    "\n",
    "Run the lines below to randomly split the data into a training (2/3 of the data) and test (1/3 of the data) sets. The proportion of samples kept for testing is a free parameter that usually ranges between 1/10 and 1/3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create matrix of data and labels\n",
    "X = bbbc010_img.loc[:, bbbc010_img.columns != 'label']\n",
    "Y = bbbc010_img.loc[:, 'label']\n",
    "\n",
    "# Portion of the data for the test set\n",
    "split = float(1/3)\n",
    "\n",
    "# Randomly split the data into training and test sets\n",
    "train_data, test_data, train_labels, test_labels = train_test_split(X, Y, test_size = split)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5.2** Support Vector Machines (SVM, https://en.wikipedia.org/wiki/Support-vector_machine) is a family of classical machine learning algorithms that attempts at finding separation between classes as hyperplanes that are as far as possible to the data points. This is done by maximizing the length of the vector from the closest data point that is orthogonal to the class separation boundary, which is referred to as *support vector* and gives the algorithm its name. \n",
    "\n",
    "While the technical details of SVM are out of the scope of this course, we can get a feel for the power of this approach if we try to classify our data as they were originally, without applying any feature selection or dimensionality reduction method. Run the lines below to train a linear SVM on the training set and use it to predict the labels of the test set. How does it look?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create matrix of data and labels\n",
    "X = train_data\n",
    "Y = train_labels\n",
    "\n",
    "# Train a linear SVM on the training set\n",
    "classifier = svm.SVC()\n",
    "classifier.fit(X, Y)\n",
    "\n",
    "# Predict labels on the test set\n",
    "pred_labels = classifier.predict(test_data)\n",
    "\n",
    "# Compute the confusion matrix\n",
    "confusion_matrix = skm.confusion_matrix(test_labels, pred_labels, labels = np.unique(test_labels))\n",
    "\n",
    "# Visualize the result\n",
    "disp = skm.ConfusionMatrixDisplay(confusion_matrix = confusion_matrix, display_labels = np.unique(test_labels))\n",
    "disp.plot()\n",
    "\n",
    "plt.show()\n",
    "# A perfect classification result!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5.3** To compare against SVM, let's try and see how K-means would have fared if it had to deal with the original feature matrix. Adapt the code from 3.1 and 4.1 to run K-means on the whole dataset without dimensionality reduction, and compare with both the K-means result you obtained in 4.1 and the SVM result you obtained in 5.2. What do you notice?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# There are (still!) two classes in this dataset\n",
    "K = 2\n",
    "\n",
    "# Create matrix of data and labels\n",
    "X = bbbc010_img.loc[:, bbbc010_img.columns != 'label']\n",
    "Y = bbbc010_img.loc[:, 'label']\n",
    "\n",
    "# Perform K-means on the test data\n",
    "kmeans = KMeans(n_clusters=K, random_state=5)\n",
    "kmeans.fit(X)\n",
    "\n",
    "# Retreive the cluster label and display it\n",
    "kmeans_labels = kmeans.predict(X)\n",
    "\n",
    "predictions = np.empty((len(kmeans_labels)), dtype='object')\n",
    "predictions[kmeans_labels == 1] = 'live'\n",
    "predictions[kmeans_labels == 0] = 'dead'\n",
    "\n",
    "# Compute the confusion matrix\n",
    "confusion_matrix = skm.confusion_matrix(Y, predictions, labels = np.unique(Y))\n",
    "\n",
    "# Visualize the result\n",
    "disp = skm.ConfusionMatrixDisplay(confusion_matrix = confusion_matrix, display_labels = np.unique(Y))\n",
    "disp.plot()\n",
    "\n",
    "plt.show()\n",
    "# We are making one mistake when skipping dimensionality reduction with K-means!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5.4** Another popular classical supervised machine learning algorithm is decision trees (https://en.wikipedia.org/wiki/Decision_tree). Decision trees, as their name imply, perform classification by identifying sequences of logical rules to be followed in order to decide which class a sample should be assigned to. In addition to being extremely fast to train, like SVM, decision trees have the interesting property of being fully interpretable. Random forests, as used in ilastik's pixel classification workflow for segmentation (https://www.ilastik.org/), are based on decision trees.\n",
    "\n",
    "Run the code below to 1) train a decision tree on the training set and retreive predictions on the test set, and 2) visualize the decision tree. Based on this, can you identify which feature(s) is(are) used to decide on the class?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create matrix of data and labels\n",
    "X = train_data\n",
    "Y = train_labels\n",
    "\n",
    "# Train decision tree on the training set\n",
    "classifier = tree.DecisionTreeClassifier()\n",
    "classifier.fit(X, Y)\n",
    "\n",
    "# Retreive predictions on the test set\n",
    "pred_labels = classifier.predict(test_data)\n",
    "\n",
    "# Compute the confusion matrix\n",
    "confusion_matrix = skm.confusion_matrix(test_labels, pred_labels, labels = np.unique(test_labels))\n",
    "\n",
    "# Visualize the result\n",
    "disp = skm.ConfusionMatrixDisplay(confusion_matrix = confusion_matrix, display_labels = np.unique(test_labels))\n",
    "disp.plot()\n",
    "\n",
    "plt.show()\n",
    "# An almost-perfect classification result!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the decision tree\n",
    "tree.plot_tree(classifier)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the feature used for decision (X[n])\n",
    "train_data.columns[4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5.5** The decision tree you trained in 5.4 provides you with a feature and a threshold to classify the data. Try to write a piece of code that assigns class labels by manually thresholding that one feature identified by the decision trees, and check the result. Does that work?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thresh = 0.682\n",
    "index = 4\n",
    "\n",
    "X = test_data.loc[:, train_data.columns[index]]\n",
    "\n",
    "predictions = np.empty((len(X)), dtype='object')\n",
    "predictions[X<=thresh]= 'live'\n",
    "predictions[X>thresh]= 'dead'\n",
    "\n",
    "# Retreive predictions (PCA-transformed data, 1st PC) and ground truth\n",
    "ground_truth = test_labels\n",
    "\n",
    "# Compute the confusion matrix\n",
    "confusion_matrix = skm.confusion_matrix(ground_truth, predictions, labels = np.unique(ground_truth))\n",
    "\n",
    "# Visualize the result\n",
    "disp = skm.ConfusionMatrixDisplay(confusion_matrix = confusion_matrix, display_labels = np.unique(ground_truth))\n",
    "disp.plot()\n",
    "\n",
    "plt.show()\n",
    "# As expected, we get the same result as the decision tree. The decision tree has thus helped us identify a \n",
    "# single feature that allows discriminating strongly between our two classes, and a threshold we can use for\n",
    "# that purpose."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BONUS. Classifying individual objects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you like challenges, apply everything you've seen up to now to try and classify individual *C. elegans* worms in the dataset instead of whole images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**6.1** Run the lines below to load and display the feature matrix for the entire BBBC010 dataset. This time, features are reported for individual object instances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bbbc010_obj_feats = pd.read_csv('../data/Part_4/BBBC010/bbbc010_object_features.csv')\n",
    "bbbc010_obj_feats.set_index('instance_id', inplace = True)\n",
    "\n",
    "display(bbbc010_obj_feats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**6.2** Run the lines below to load and display the ground truth label for each instance in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bbbc010_obj_gt = pd.read_csv('../data/Part_4/BBBC010/bbbc010_object_ground_truth.csv')\n",
    "bbbc010_obj_gt.set_index('instance_id', inplace = True)\n",
    "\n",
    "display(bbbc010_obj_gt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**6.3** Classify individual worms in the dataset as \"dead\" or \"live\", and report the performance of your solution using appropriate metrics. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a challenge, no solution given here :) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**6.4** Based on your classification results, identify images that contain a mix of dead and live worms. Relating your observations with the whole-image label they had in bbbc010_img_gt (1.3), would you say that these images were mislabeled? Why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a challenge, no solution given here :) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
