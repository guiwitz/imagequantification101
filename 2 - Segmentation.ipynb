{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Segmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we will look at some basic classical (non-machine-learning) methods to partition an image into objects of interest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import imageio.v2 as imageio\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.rcParams['figure.dpi'] = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import skimage as sk\n",
    "import skimage.filters as skf\n",
    "import skimage.measure as skm\n",
    "import skimage.morphology as sku"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will work with images from the BBBC datasets considered in notebook 1 - Handling Image Data. For each dataset, we will consider examples of an \"easier\" segmentation problem. The \"harder\" samples should be left aside for now and until section 5."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.1** Using what you learned in notebook 1 - Handling Image Data, load the following image files:\n",
    "- data/Part 2/BBBC010/easier.tif\n",
    "- data/Part 2/BBBC010/harder.tif\n",
    "- data/Part 2/BBBC020/easier.tif\n",
    "- data/Part 2/BBBC020/harder.tif\n",
    "\n",
    "*Do not forget to convert the images into floating-point arrays with img_as_float!*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BBBC010\n",
    "##### Add you code here ##### \n",
    "\n",
    "# BBBC020\n",
    "##### Add you code here ##### "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.2** Using what you learned in notebook 1 - Handling Image Data, vizualize the images you will be working with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BBBC010\n",
    "##### Add you code here ##### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BBBC020\n",
    "##### Add you code here ##### "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Semantic segmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a first step, we will attempt to group pixels into two broad \"background\" (containing no relevant information) and \"foreground\" (containing the objects of interest) categories. This is referred to as *semantic segmentation*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.1** The simplest way to segment an image is to identify a cutoff value that separates pixels belonging to the background and foreground. This is referred to as *thresholding*. To help us find a threshold value, observe the  histogram of the BBBC020 image obtained by running the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot histogram\n",
    "plt.hist(bbbc020_easy.ravel())\n",
    "plt.xlabel(\"Pixel values\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.2** Based on the histogram, choose a threshold value. Then, complete the code below to retreive all pixels with values larger or equal than this threshold. The resulting binary image is referred to as a *mask*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set threshold \n",
    "##### Add you code here ##### \n",
    "\n",
    "# Retreive values larger or equal\n",
    "##### Add you code here ##### \n",
    "\n",
    "# Visualize the result\n",
    "##### Add you code here ##### "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.3** The process of finding a threshold value can be automated. Several methods are available to automatically retreive a threshold value, but since they are based on different criteria this value may differ. Try out a few automated thresholding algorithms such as threshold_otsu, threshold_mean, and threshold_minimum from https://scikit-image.org/docs/dev/api/skimage.filters.html. Do you see any difference? How does it compare to what you obtained by thresholding manually? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_img = bbbc020_easy\n",
    "\n",
    "# Otsu's threshold\n",
    "##### Add you code here ##### \n",
    "\n",
    "# Mean threshold\n",
    "##### Add you code here ##### \n",
    "\n",
    "# Minimum threshold\n",
    "##### Add you code here ##### \n",
    "\n",
    "# Visualize the result\n",
    "##### Add you code here ##### )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.4** If you did not find it in 2.3 already, note that scikit-image provides a try_all_threshold function that does extractly what its name implies. Run it and observe the result. According to you, which method provides the best results? Why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = skf.try_all_threshold(bbbc020_easy)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Instance segmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've identified which parts of the image belong to foreground, we may want to separate it into individual objects. This is referred to as *instance segmentation*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.1** Individual object instances can be retreived by assigning an identical value to foreground pixels that are physically connected, a method referred to as *connected component labeling* (https://en.wikipedia.org/wiki/Connected-component_labeling). Pick you favourite thresholding method and complete the code below to see the result of connected component labeling on the mask of BBBC020. \n",
    "\n",
    "*Bonus:* one important parameters of connected component labeling is the definition of connectivity, which can be adapted in the label function below. How does the end result differ? Do you understand why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_img = bbbc020_easy\n",
    "\n",
    "# Threshold\n",
    "##### Add you code here ##### \n",
    "\n",
    "# Save this semantic mask for later\n",
    "bbbc020_easy_sem_mask = thresholded_img\n",
    "\n",
    "# Label connected components\n",
    "instance_labels = skm.label(mask_img, background=0, connectivity=2)\n",
    "\n",
    "# Save this instance mask for later\n",
    "bbbc020_easy_inst_mask = instance_labels\n",
    "\n",
    "# Visualize the result\n",
    "fig, axes = plt.subplots(nrows=1, ncols=3)\n",
    "\n",
    "axes[0].imshow(bbbc020_easy, cmap='gray')\n",
    "axes[0].axis('off')\n",
    "axes[0].set_title('Original image', fontsize=9)\n",
    "\n",
    "axes[1].imshow(mask_img, cmap='gray')\n",
    "axes[1].axis('off')\n",
    "axes[1].set_title('Mask', fontsize=9)\n",
    "\n",
    "axes[2].imshow(bbbc020_easy, cmap='gray')\n",
    "axes[2].imshow(instance_labels, cmap='nipy_spectral', alpha=.5)\n",
    "axes[2].axis('off')\n",
    "axes[2].set_title('Instances', fontsize=9)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.2** Alternatively to instance masks, individual objects can also be described by the contour of the area they occupy.  Pick you favourite thresholding method and complete the code below to see the result of an automated contour-finding algorithm (marching squares, https://en.wikipedia.org/wiki/Marching_squares) on the mask of BBBC020. Observe the result carefully: what do you notice? Where do you think this is coming from?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "original_img = bbbc020_easy\n",
    "\n",
    "# Threshold\n",
    "##### Add you code here ##### \n",
    "\n",
    "# Extract contours\n",
    "contours = skm.find_contours(thresholded_img, level=0)\n",
    "\n",
    "# Visualize the result\n",
    "fig, axes = plt.subplots(nrows=1, ncols=3)\n",
    "\n",
    "axes[0].imshow(original_img, cmap='gray')\n",
    "axes[0].axis('off')\n",
    "axes[0].set_title('Original image', fontsize=9)\n",
    "\n",
    "axes[1].imshow(mask_img, cmap='gray')\n",
    "axes[1].axis('off')\n",
    "axes[1].set_title('Mask', fontsize=9)\n",
    "\n",
    "axes[2].imshow(original_img, cmap='gray')\n",
    "axes[2].axis('off')\n",
    "axes[2].set_title('Overlaid contours', fontsize=9)\n",
    "\n",
    "for c in contours:\n",
    "    axes[2].scatter(c[:,1],c[:,0], s=.1, marker='.')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "##### What do you notice? #####\n",
    "plt.imshow(original_img, cmap='gray')\n",
    "plt.axis('off')\n",
    "plt.title('Look closely!', fontsize=9)\n",
    "\n",
    "for c in contours:\n",
    "    plt.scatter(c[:,1],c[:,0], s=1, marker='.')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.3** In the early days of image processing, researchers developed an extensive framework to manipulate binary masks called mathematical morphology (https://en.wikipedia.org/wiki/Mathematical_morphology). Although the theory behind mathematical morphology is out of the scope of this tutorial, a rough summary of the idea behind it is to probe a binary mask with a simple shape called *structuring element* and characterize whether this shape fits or misses the foreground objects.\n",
    "\n",
    "The spurious contours we observed in 3.2 are due to isolated noisy pixels left by thresholding in our segmentation mask. They can be easily removed using mathematical morphology: considering a structural element that is a disk with a radius of a few pixels, we will erase all parts of the foreground that cannot fit it. As a result, the contours of each object instance will be smoothed out.\n",
    "\n",
    "Run the code below to see how this so-called *opening* operation affects the detected contours. Try out different structural element sizes (the disk_size parameter) and observe the result. What do you see? Do you intuitively understand why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_img = bbbc020_easy\n",
    "\n",
    "# Threshold\n",
    "##### Add you code here ##### \n",
    "\n",
    "# Mathematical morphology\n",
    "disk_size = 5 ##### Try to modify this #####\n",
    "structuring_element = sku.disk(disk_size)\n",
    "clean_mask = sku.binary_opening(mask_img, structuring_element)\n",
    "\n",
    "# Extract contours\n",
    "contours = skm.find_contours(clean_mask, level=0)\n",
    "\n",
    "# Visualize the result\n",
    "fig, axes = plt.subplots(nrows=1, ncols=3)\n",
    "\n",
    "axes[0].imshow(original_img, cmap='gray')\n",
    "axes[0].axis('off')\n",
    "axes[0].set_title('Original image', fontsize=9)\n",
    "\n",
    "axes[1].imshow(clean_mask, cmap='gray')\n",
    "axes[1].axis('off')\n",
    "axes[1].set_title('Clean mask', fontsize=9)\n",
    "\n",
    "axes[2].imshow(original_img, cmap='gray')\n",
    "axes[2].axis('off')\n",
    "axes[2].set_title('Overlaid contours', fontsize=9)\n",
    "\n",
    "for c in contours:\n",
    "    axes[2].scatter(c[:,1],c[:,0], s=.1, marker='.')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "##### What do you notice? #####\n",
    "plt.imshow(original_img, cmap='gray')\n",
    "plt.axis('off')\n",
    "plt.title('Look closely!', fontsize=9)\n",
    "\n",
    "for c in contours:\n",
    "    plt.scatter(c[:,1],c[:,0], s=1, marker='.')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.4** Combining the methods we looked at so far, try to instance-segment the BBBC010 image. \n",
    "\n",
    "*Hint: remember that the image background can be subtracted by making use of Gaussian filtering, as seen in notebook 1 - Handling Image Data!*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_img = bbbc010_easy\n",
    "\n",
    "##### Add you code here ##### "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Evaluating segmentation quality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we are able to produce semantic and instance segmentation masks, we may rightfully ask ourselves how good they are. Here, we will look at classical metrics that allow evaluating the quality of a segmentation in a quantitative manner."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4.1** In order to evaluate segmentation quality, we need a reference of what the \"perfect\" result would look like. This is referred to as a *ground truth*, and is generally obtained through manual curation. Run the lines below to load the semantic ground truth masks corresponding to the images we are working with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BBBC010\n",
    "bbbc010_easy_sem_gt = imageio.imread('data/Part 2/BBBC010/easier_semantic_ground_truth.tif')\n",
    "bbbc010_easy_sem_gt = (bbbc010_easy_sem_gt>0)\n",
    "bbbc010_hard_sem_gt = imageio.imread('data/Part 2/BBBC010/harder_semantic_ground_truth.tif')\n",
    "bbbc010_hard_sem_gt = (bbbc010_hard_sem_gt>0)\n",
    "\n",
    "# BBBC020\n",
    "bbbc020_easy_sem_gt = imageio.imread('data/Part 2/BBBC020/easier_semantic_ground_truth.tif')\n",
    "bbbc020_easy_sem_gt = (bbbc020_easy_sem_gt>0)\n",
    "bbbc020_hard_sem_gt = imageio.imread('data/Part 2/BBBC020/harder_semantic_ground_truth.tif')\n",
    "bbbc020_hard_sem_gt = (bbbc020_hard_sem_gt>0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4.2** Adapting what you learned in Part 3, vizualize the semantic ground truth masks overlaid on top of the original images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BBBC010\n",
    "##### Add you code here ##### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BBBC020\n",
    "##### Add you code here ##### "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4.3** The Intersection-over-Union (IoU) and Dice scores are two commonly-used semantic segmentation metrics ranging between 0 and 1, with higher values corresponding to better results. Looking at their definition in the function below, can you intuitively understand what these metrics measure?\n",
    "\n",
    "Using the instance_segmentation_metrics function, rank the segmentation masks you obtained in Part 3 based on the IoU and Dice scores. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def instance_segmentation_metrics(mask_computed, mask_gt):\n",
    "    # Intersection = logical AND\n",
    "    overlap = mask_computed*mask_gt \n",
    "    # Union = logical OR\n",
    "    union = mask_computed + mask_gt \n",
    "\n",
    "    IoU = overlap.sum()/float(union.sum())\n",
    "    Dice = 2*overlap.sum()/(float(union.sum()+overlap.sum()))\n",
    "\n",
    "    return (IoU, Dice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BBBC010\n",
    "##### Add you code here ##### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BBBC020\n",
    "##### Add you code here ##### "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4.4** In addition to a semantic ground truth, we can also define an instance-based ground truth. Run the lines below to load the instance ground truth masks corresponding to the images we are working with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BBBC010\n",
    "bbbc010_easy_inst_gt = imageio.imread('data/Part 2/BBBC010/easier_instance_ground_truth.tif')\n",
    "bbbc010_hard_inst_gt = imageio.imread('data/Part 2/BBBC010/harder_instance_ground_truth.tif')\n",
    "\n",
    "# BBBC020\n",
    "bbbc020_easy_inst_gt = imageio.imread('data/Part 2/BBBC020/easier_instance_ground_truth.tif')\n",
    "bbbc020_hard_inst_gt = imageio.imread('data/Part 2/BBBC020/harder_instance_ground_truth.tif')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4.5** Adapting what you learned in Part 3, vizualize the instance ground truth masks overlaid on top of the original images. \n",
    "\n",
    "An instance ground truth is usually provided as a set of images that each correspond to the segmentation mask of an individual object. Looking at the harder examples, can you guess why having an individual masks for each object may be beneficial?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BBBC010\n",
    "##### Add you code here ##### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BBBC020\n",
    "##### Add you code here ##### "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4.6** Semantic segmentation metrics such as the IoU and Dice can also be used to assess the segmentation quality of individual instances. Additional metrics that can be useful to evaluate the performance of an instance segmentation result are those that assess the performance of detection. Precision and recall are such examples. Looking at their definition in the function below, can you intuitively understand what these metrics measure?\n",
    "\n",
    "Using the semantic_segmentation_metrics function, rank the segmentation masks you obtained in Part 3 based on the precision and recall. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def semantic_segmentation_metrics(mask_computed, mask_gt):\n",
    "    # Recover instance labels\n",
    "    instances_gt = np.unique(mask_gt)\n",
    "    instances_gt = np.delete(instances_gt,0)\n",
    "\n",
    "    instances_computed = np.unique(mask_computed)\n",
    "    instances_computed  = np.delete(instances_computed ,0)\n",
    "    \n",
    "    # Recover overlaps \n",
    "    match = {}\n",
    "    for i in instances_gt:\n",
    "        mask_i = (mask_gt==i)\n",
    "        \n",
    "        for j in instances_computed:\n",
    "            mask_j = (mask_computed==j)\n",
    "            \n",
    "            intersection = mask_i*mask_j\n",
    "            overlap = intersection.sum()\n",
    "            \n",
    "            # Here we choose to pick the instance with the largest overlap as a match\n",
    "            if overlap>0:\n",
    "                if i in match:\n",
    "                    if match[i][1]>overlap:\n",
    "                        match[i]=[j,overlap]\n",
    "                else:\n",
    "                    match[i]=[j,overlap]\n",
    "               \n",
    "    matched_gt = [k for k in match.keys()]\n",
    "    matched_computed = [v[0] for v in match.values()]\n",
    "    \n",
    "    # True positive = GT instances that overlap \n",
    "    TP = len(list(set(matched_gt) & set(instances_gt)))\n",
    "    \n",
    "    # False positive = computed instances that have no match in GT\n",
    "    FP = len(instances_computed) - len(list(set(matched_computed) & set(instances_computed)))\n",
    "    \n",
    "    # False negative = GT instances that have no match in the computed mask\n",
    "    FN = len(instances_gt) - len(list(set(matched_gt) & set(instances_gt)))\n",
    "\n",
    "    precision = TP/(TP+FP)\n",
    "    recall = TP/(TP+FN)\n",
    "\n",
    "    return (precision, recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BBBC010\n",
    "##### Add you code here ##### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BBBC020\n",
    "##### Add you code here ##### "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BONUS. How well can you segment?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you like challenges, apply everything you've seen up to now to try and segment the hard images. Report the quality of your result using the metrics above, and compare with your colleagues!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5.1** Segment the hard example from BBBC010."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_img = bbbc010_hard\n",
    "\n",
    "##### Add you code here ##### "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5.2** Segment the hard example from BBBC020."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_img = bbbc020_hard\n",
    "\n",
    "##### Add you code here ##### "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
